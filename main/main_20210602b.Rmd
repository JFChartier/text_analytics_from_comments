---
title: "Modélisation des commentaires des agents en assurance de la personne"
author: "Jean-Francois Chartier"
date: "`r Sys.Date()`"
output: 
  html_document: 
    code_folding: hide
    fig_width: 5
    highlight: tango
    theme: united
    toc: yes
    toc_float: yes
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# lib
```{r}
library(magrittr)
#library(data.table)
library(stringr)
library(quanteda)
library(dplyr)
library(qdapRegex)
library(text2vec)
require(tidyverse)
require(rjson)

#source("textPreprocessingFunctions.R")
source("~/datalab-nlp/JF Chartier/SemanticModels/TextModeling/LSA/textPreprocessingFunctions.R")

```

Préparation des données

## Lecture des données
```{r}

#svdFromWiki300 = readRDS("~/datalab-nlp/JF Chartier/Data/SemanticModels/matrixV_svdFromWiki300.rds")

svdFromWiki300 = readRDS("~/datalab-nlp/JF Chartier/Data/SemanticModels/FrenchWikiTokensAnd4gramsNormed300SVD.rds")

myFrenchStopwords = data.table::fread("~/datalab-nlp/JF Chartier/Data/SemanticModels/antidictionnaireFrancais.csv", header = F, encoding = "UTF-8")

comments_from_agents = data.table::fread("~/datalab-nlp/JF Chartier/Data/text_analytics_from_comments/comments_from_calls_2020.csv", header = T, encoding = "UTF-8")

fr_dic = hunspell::dictionary("~/datalab-nlp/JF Chartier/Data/SemanticModels/Dictionaries/fr_FR.dic")

token_to_lemme_fr = readr::read_tsv('http://www.lexique.org/databases/Lexique383/Lexique383.tsv') %>% .[, c("ortho", "lemme")]
#voir alternative morphalou https://github.com/chrplr/openlexicon/blob/master/datasets-info/Morphalou/README-Morphalou.md

```


## Prétraitement des commentaires
```{r}
#french_dic = dictionary("~/.config/rstudio/dictionaries/languages-system/fr_FR.dic")
#hunspell::list_dictionaries()


comments_from_agents = comments_from_agents[comments_from_agents$Langue == "Français", ]
comments_from_agents = comments_from_agents[nchar(comments_from_agents$Commentaires_de_lagent) > 3 , ]


#text_preprocessing_results = text_preprocessing(text_in_claims = comments_from_agents$Commentaires_de_lagent, myFrenchStopwords = myFrenchStopwords, my_doc_count_min = 1, my_doc_proportion_max = 1, to_stemm = T, language = "fr", spelling_checker = F, charNgram = 4, remove_accent = T, min_token_length = 3)


tokenizedCorpus = myTextCleaner(comments_from_agents$Commentaires_de_lagent) %>% myTextTokenizer(text = ., charNgram = 4, myFrenchStopwords = myFrenchStopwords, to_lemmetize = T, to_stemm = T, language = "fr", spelling_checker = F, remove_accent = T, min_token_length = 3, dictionary = NULL, token_to_lemme = token_to_lemme_fr, word_ngram = 1:2) 

vectorizedCorpus = myTextVectorizer(tokenizedCorpus = tokenizedCorpus, my_doc_count_min = 3, my_doc_proportion_max = .5)

text_preprocessing_results = list(tokenizedCorpus=tokenizedCorpus, vectorizedCorpus=vectorizedCorpus)
```


## Prétraitement des catégories
```{r}
categorie_appel = comments_from_agents$Categorie_dappel %>% tolower() %>% stringr::str_squish()
# filtrer les categorie rares
categorie_appel_freq = table(categorie_appel) %>% is_less_than(5) %>% table(categorie_appel)[.] %>% names(.) #%>% categorie_appel[categorie_appel %in% .]
categorie_appel[categorie_appel %in% categorie_appel_freq] = "category_less_frequent_that_5"

tokens_from_cat = lapply(unique(categorie_appel), function(x){
  #x = unique(categorie_appel)[1]
  #print(x)
  #specificities = get_most_relevant_words(tokenizedCorpus = text_preprocessing_results$tokenizedCorpus, target = categorie_appel==x, minDocFrequency = 0, coefficient = "lr", min_specificity_score = 0)
  
  tokens_in_cat = c(text_preprocessing_results$tokenizedCorpus[categorie_appel==x]) %>% unlist()
  #tokens_in_cat = tokens_in_cat[tokens_in_cat %in% specificities]
  return(tokens_in_cat)
})

# specificity_reee = get_most_relevant_words(tokenizedCorpus = tokens_from_cat, target = unique(categorie_appel)=="reee", n_top_word = 100, minDocFrequency = 0, coefficient = "lr")
# View(specificity_reee, title = "group")
# 
# specificity_reee = get_most_relevant_words(tokenizedCorpus = text_preprocessing_results$tokenizedCorpus, target = (categorie_appel)=="reee", n_top_word = 100, minDocFrequency = 0, coefficient = "lr")
# 
# View(specificity_reee, title = "single")




#vector_of_categorieAppel = myTextVectorizer(tokenizedCorpus = (tokens_from_cat), my_doc_count_min = 0, my_doc_proportion_max = 1) %>% set_rownames(unique(categorie_appel)) #%>% scale(., center = T, scale = F)

vector_of_categorieAppel = quanteda::dfm_group(text_preprocessing_results$vectorizedCorpus %>% quanteda::as.dfm(), groups= categorie_appel)

```

## Prétraitement des mots
```{r}

#unique_word_in_comments = myTextCleaner(comments_from_agents$Commentaires_de_lagent) %>% myTextTokenizer(text = ., charNgram = 0, myFrenchStopwords = myFrenchStopwords, to_stemm = F, language = "fr", spelling_checker = F, remove_accent = T, min_token_length = 3) %>% unlist() %>% unique()

# word speller ne fonctionne pas
unique_word_in_comments = myTextCleaner(comments_from_agents$Commentaires_de_lagent) %>% myTextTokenizer(text = ., charNgram = 0, myFrenchStopwords = myFrenchStopwords, to_stemm = F, language = "fr", spelling_checker = F, remove_accent = T, min_token_length = 3, to_lemmetize = T, dictionary = fr_dic, token_to_lemme = token_to_lemme_fr, word_ngram = NULL) %>% unlist() %>% unique()


#unique_word_preprocessed = text_preprocessing(text_in_claims = unique_word_in_comments, myFrenchStopwords = NULL, my_doc_count_min = 0, my_doc_proportion_max = 1, to_stemm = T, language = "fr", spelling_checker = F, charNgram = 4, remove_accent = F, min_token_length = 0) #sapply(unique_word_in_comments, function(x) paste("dummy", x, "dummy", sep = " ")) %>% 

unique_word_vectorized = myTextCleaner(unique_word_in_comments) %>% myTextTokenizer(text = ., charNgram = 0, myFrenchStopwords = NULL, to_lemmetize = F, to_stemm = T, language = "fr", spelling_checker = F, remove_accent = F, min_token_length = 0, dictionary = NULL, token_to_lemme = NULL, word_ngram = NULL) %>% myTextVectorizer(tokenizedCorpus = ., my_doc_count_min = 0, my_doc_proportion_max = 1)
```



# Modèle LSA basé sur Wikipédia

## Application de la LSA
```{r}

reduced_categories_space = getLatentVectorsOfComments(vectorizedCorpus = vector_of_categorieAppel, svd_v = svdFromWiki300$v, svd_d = svdFromWiki300$d, originalFeatures = svdFromWiki300$original.features)

reduced_comments_space = getLatentVectorsOfComments(vectorizedCorpus = text_preprocessing_results$vectorizedCorpus, svd_v = svdFromWiki300$v, svd_d = svdFromWiki300$d, originalFeatures = svdFromWiki300$original.features)
 

reduced_word_space = getLatentVectorsOfComments(vectorizedCorpus = unique_word_vectorized, svd_v = svdFromWiki300$v, svd_d = svdFromWiki300$d, originalFeatures = svdFromWiki300$original.features)
rownames(reduced_word_space) = unique_word_in_comments

```




# Modèle LSA à partir des commentaires

## Application de la LSA
```{r}
svd_comments = RSpectra::svds(text_preprocessing_results$vectorizedCorpus %>% text2vec::normalize(., "l2"), k = 200)
svd_comments$original.features = text_preprocessing_results$vectorizedCorpus %>% colnames()

reduced_categories_space_from_svdComments = getLatentVectorsOfComments(vectorizedCorpus = vector_of_categorieAppel, svd_v = svd_comments$v, svd_d = svd_comments$d, originalFeatures = svd_comments$original.features)

reduced_word_space_from_svdComments = getLatentVectorsOfComments(vectorizedCorpus = unique_word_vectorized, svd_v = svd_comments$v, svd_d = svd_comments$d, originalFeatures = svd_comments$original.features)
rownames(reduced_word_space_from_svdComments) = unique_word_in_comments

reduced_comments_space_from_svdComment = getLatentVectorsOfComments(vectorizedCorpus = text_preprocessing_results$vectorizedCorpus, svd_v = svd_comments$v, svd_d = svd_comments$d, originalFeatures = svd_comments$original.features)

```


# Modèle LSA combinées

## Préparation des modèles
```{r}
#v_appended = cbind(svd_comments$v, svdFromWiki300$v[svdFromWiki300$original.features %in% svd_comments$original.features, ])

rownames(svd_comments$v) = svd_comments$original.features
rownames(svdFromWiki300$v) = svdFromWiki300$original.features

v_appended = merge(x = svd_comments$v, by="row.names", all.x = T, y = svdFromWiki300$v[,1:100], all.y=T, sort = FALSE)
v_appended[is.na(v_appended)] = 0

features_appended = v_appended$Row.names
v_appended$Row.names=NULL
#ici c'est très important de normer les vecteurs d, car il ont une magnitude très différente
d_appended = c(normVector(svd_comments$d), normVector(svdFromWiki300$d[1:100]))

svd_appended = list(d = d_appended, v=as.matrix(v_appended), original.features = features_appended)

```

## Application de la LSA
```{r}
# vectorizedCorpus = quanteda::as.dfm(vector_of_categorieAppel)
# vectorizedCorpus = quanteda::dfm_match(vector_of_categorieAppel, features = svd_appended$original.features)
#  myReducedQueryVector <-  vectorizedCorpus %*% svd_appended$v %*% solve(diag((svd_appended$d)))  
# 

reduced_categories_space_from_svdAppended = getLatentVectorsOfComments(vectorizedCorpus = vector_of_categorieAppel, svd_v = svd_appended$v, svd_d = svd_appended$d, originalFeatures = svd_appended$original.features)

reduced_word_space_from_svdAppended = getLatentVectorsOfComments(vectorizedCorpus = unique_word_vectorized, svd_v = svd_appended$v, svd_d = svd_appended$d, originalFeatures = svd_appended$original.features)
rownames(reduced_word_space_from_svdAppended) = unique_word_in_comments

reduced_comments_space_from_svdAppended = getLatentVectorsOfComments(vectorizedCorpus = text_preprocessing_results$vectorizedCorpus, svd_v = svd_appended$v, svd_d = svd_appended$d, originalFeatures = svd_appended$original.features)

```

## Comparaison entre mots et catégories


### Sélectionner top 25 catégories
```{r}

reduced_categories_space_from_svdAppended_top = reduced_categories_space_from_svdAppended[rownames(reduced_categories_space_from_svdAppended) %in% (table(categorie_appel) %>% sort(., decreasing = T) %>% .[1:25] %>% names()), ]

similarity_between_cat_word_append=text2vec::sim2(x = as.matrix(reduced_word_space_from_svdAppended), y=as.matrix(reduced_categories_space_from_svdAppended_top), method = "cosine", norm = "none") #%>% 
View((similarity_between_cat_word_append), title = "appended_LSA_6")

```

### Bar plot 
```{r}
similarity_between_cat_word_append_long = reshape2::melt(data = similarity_between_cat_word_append) %>% set_colnames(c("word", "category", "correlation"))

top.word.by.cat <- similarity_between_cat_word_append_long %>%
  dplyr::group_by(category) %>%
  dplyr::top_n(15, correlation) %>%
  dplyr::ungroup() %>%
  dplyr::arrange(category, desc(correlation))


top.word.by.cat %>%
  #mutate(topic = reorder(word, correlation)) %>%
  ggplot(data = ., aes(x = tidytext::reorder_within(word, correlation, within = category), y = correlation, fill = category)) +
  #ggplot(data = ., aes(x = word, y = correlation, fill = category)) +
  
  geom_col(show.legend = F, color="black", size=0.1)+
  facet_wrap(~ category, scales = "free", ncol=5) +
  coord_flip()+
  tidytext::scale_x_reordered() +
  #geom_text(color = "black",  size=1, hjust=0, vjust=0)+
  ggtitle("Top-15 words by category")+
  #theme_light(base_size = 6)+
  theme(axis.text=element_text(size=6, colour = "black"),
        axis.title=element_text(size=6, colour = "black"),
        title=element_text(size=6, colour = "black"))

```


### wordclouds
```{r}
# word_cor_by_cat = lapply(unique(top.word.by.cat$category), FUN = function(c){
#   dat= top.word.by.cat[top.word.by.cat$category==c, ] %>% .[, c("word", "correlation")]
#   dat$correlation = (dat$correlation * dat$correlation)#*10
#   return(dat)
# })
# par(mfrow=c(1,1)) # for 5 row, 5 cols
# for (i in 1:length(word_cor_by_cat)){
#   
#   i=19
#   wordcloud2::wordcloud2(word_cor_by_cat[[i]])
#   #wordcloud::wordcloud(words = word_cor_by_cat[[i]]$word, freq = word_cor_by_cat[[i]]$correlation)
# }



set.seed(42)
top.word.by.cat$correlation2 = top.word.by.cat$correlation^2
ggplot(top.word.by.cat, aes(label = word, size = correlation2, color=category)) +
  ggwordcloud::geom_text_wordcloud_area() +
  scale_size_area(max_size = 15) +
  theme_minimal() +
  facet_wrap(~category, scales = "free", ncol=4)
```

### wordspace
```{r}
library(Rtsne)
set.seed(9)

pca_x = prcomp(reduced_word_space_from_svdAppended, center = T, scale. = T)

#incredible tse!
tsne.word= Rtsne::Rtsne(reduced_word_space_from_svdAppended, check_duplicates=FALSE, pca=F,perplexity=50, theta=0.05, dims=2, pca_center=F, pca_scale=F, exaggeration_factor=12, stop_lying_iter=400, max_iter=1500, num_threads=6)

tsne.word.1= Rtsne::Rtsne(reduced_word_space_from_svdAppended, check_duplicates=FALSE, pca=F,perplexity=10, theta=0.5, dims=2, pca_center=F, pca_scale=F, exaggeration_factor=12, stop_lying_iter=400, max_iter=1500, num_threads=6)

tsne.word.2= Rtsne::Rtsne(reduced_word_space_from_svdAppended, check_duplicates=FALSE, pca=T, perplexity=50, theta=0.5, dims=2, pca_center=T, pca_scale=T, exaggeration_factor=12, stop_lying_iter=400, max_iter=1500, num_threads=6)

tsne.word.3= Rtsne::Rtsne(reduced_word_space_from_svdAppended, check_duplicates=FALSE, pca=T, perplexity=5, theta=0.5, dims=2, pca_center=T, pca_scale=T, exaggeration_factor=12, stop_lying_iter=400, max_iter=1500, num_threads=6)

tsne.word.4= Rtsne::Rtsne(reduced_word_space_from_svdAppended, check_duplicates=FALSE, pca=T, perplexity=30, theta=0.5, dims=2, pca_center=T, pca_scale=T, exaggeration_factor=24, stop_lying_iter=400, max_iter=1500, num_threads=6)


#stop_lying_iter=40000, max_iter=150000

#saveRDS(tsne.word,paste0("tsne_doc.50.expand12.norm.theta05.late40k.withSVD15.150k.", Sys.Date(),".rds"))

ggplot2::qplot(tsne.word.4$Y[,1], tsne.word.4$Y[,2],main = "t-Distributed Stochastic Neighbor Embedding of Words", xlab = "Dimension x", ylab = "Dimension y")




```


### comment space
```{r}
tsne.comment.1= Rtsne::Rtsne(reduced_comments_space_from_svdAppended, check_duplicates=FALSE, pca=T, perplexity=30, theta=0.5, dims=2, pca_center=T, pca_scale=T, exaggeration_factor=24, stop_lying_iter=400, max_iter=1500, num_threads=6)

tsne.comment.2= Rtsne::Rtsne(reduced_comments_space_from_svdAppended, check_duplicates=FALSE, pca=T, perplexity=30, theta=0.5, dims=2, pca_center=T, pca_scale=T, exaggeration_factor=12, stop_lying_iter=400, max_iter=1500, num_threads=6)


tsne.comment.3 = Rtsne::Rtsne(reduced_comments_space_from_svdAppended, check_duplicates=FALSE, pca=F, perplexity=30, theta=0.5, dims=2, pca_center=T, pca_scale=T, exaggeration_factor=12, stop_lying_iter=400, max_iter=1500, num_threads=2)


ggplot2::qplot(tsne.comment.2$Y[,1], tsne.comment.2$Y[,2], color= categorie_appel, main = "t-Distributed Stochastic Neighbor Embedding of Words", xlab = "Dimension x", ylab = "Dimension y")
```



### Classification des commentaires
```{r}
sim_mat=apcluster::linSimMat(x = reduced_comments_space_from_svdAppended)
comment_clusters_ap = apcluster::apcluster(s = sim_mat, seed=9)
```



```{r}


similarity_between_cat_word_append_top = similarity_between_cat_word_append[rownames(similarity_between_cat_word_append) %in% unique(top.word.by.event$word), ]

reduced_word_space_from_svdAppended_top = reduced_word_space_from_svdAppended[rownames(reduced_word_space_from_svdAppended) %in% unique(top.word.by.event$word), ]

#plot(reduced_word_space_from_svdAppended_top[, c(1, 201)])


heatmap(t(as.matrix(similarity_between_cat_word_append_top)), scale="none") %>% plotly::ggplotly()

heatmaply

mat=t(as.matrix(similarity_between_cat_word_append_top))
heatmaply::heatmaply(mat, 
        #dendrogram = "row",
        xlab = "", ylab = "", 
        main = "",
        scale = "column",
        margins = c(60,100,40,20),
        grid_color = "white",
        grid_width = 0.00001,
        titleX = FALSE,
        hide_colorbar = TRUE,
        branches_lwd = 0.1,
        #label_names = c("Country", "Feature:", "Value"),
        fontsize_row = 5, fontsize_col = 5,
        labCol = colnames(mat),
        labRow = rownames(mat),
        heatmap_layers = theme(axis.line=element_blank())
        )


plotly::plot_ly(z = (t(similarity_between_cat_word_append_top)), type = "heatmap")

```


```{r}



#plot
top.word.by.event %>%
  #mutate(topic = reorder(feature, chi2)) %>%
  ggplot(data = ., aes(x = word, y = correlation)) +
  #ggplot(data = ., aes(x = reorder(topic, topic.weight), y = topic.weight)) +
  geom_col(show.legend = T, fill="black", color="black", size=0.1)+
  facet_wrap(~ category, scales = "free", ncol=6) +
  coord_flip()+
  #geom_text(color = "black",  size=1, hjust=0, vjust=0)+
  ggtitle("Top-10 words by category")+
  theme_light(base_size = 6)+
  theme(axis.text=element_text(size=6, colour = "black"),
        axis.title=element_text(size=6, colour = "black"),
        title=element_text(size=6, colour = "black"))

```



## Comparaison entre catégories et commentaires
```{r}

similarity_between_comments_cat_appended=text2vec::sim2(x = as.matrix(reduced_comments_space_from_svdAppended), y = as.matrix(reduced_categories_space_from_svdAppended), method = "cosine", norm = "none") 
j= colnames(similarity_between_comments_cat_appended)== "reee"  #"rap / reep" #"reee" #"rpe" #"brio"   #"rap / reep"

i=order(similarity_between_comments_cat_appended[,j], decreasing = T)[1:100]
#result=data.frame(category=colnames(similarity_between_comments_cat)[j], similarity=similarity_between_comments_cat[i,j])


DT::datatable(data = data.frame(categorie_appel_clean = categorie_appel[i], comments_from_agents[i,c("Commentaires_de_lagent")], similarity=similarity_between_comments_cat_appended[i, j]), rownames = T, filter="none", selection="none", options = list(pageLength = 20, scrollX=T), caption=paste("Commentaires les plus similaire au code: ", rownames(reduced_categories_space)[j], ""))
```


## Comparaison entre mots et commentaires
```{r}
query = "plainte fraude crime" #"fraude"  # insatisfaction

clean_query = myTextCleaner(query) %>% myTextTokenizer(text = ., charNgram = 0, myFrenchStopwords = NULL, to_stemm = F, language = NULL, spelling_checker = F, remove_accent = T, min_token_length = 0, to_lemmetize = T, dictionary = NULL, token_to_lemme = token_to_lemme_fr) %>% unlist() %>% unique() %>% paste(., collapse = " ")


query_preprocessed = text_preprocessing(text_in_claims = clean_query, myFrenchStopwords = NULL, my_doc_count_min = 0, my_doc_proportion_max = 1, to_stemm = T, language = "fr", spelling_checker = F, charNgram = 0, remove_accent = T, min_token_length = 0) #sapply(unique_word_in_comments, function(x) paste("dummy", x, "dummy", sep = " ")) %>% 


reduced_query_space_svdFromComments = getLatentVectorsOfComments(vectorizedCorpus = query_preprocessed$vectorizedCorpus, svd_v = svd_appended$v, svd_d = svd_appended$d, originalFeatures = svd_appended$original.features)
rownames(reduced_query_space_svdFromComments) = query


similarity_between_comments_query=text2vec::sim2(x = as.matrix(reduced_comments_space_from_svdAppended), y = as.matrix(reduced_query_space_svdFromComments), method = "cosine", norm = "none") 

i=order(similarity_between_comments_query[,1], decreasing = T)[1:1000]
#result=data.frame(category=colnames(similarity_between_comments_query), similarity=similarity_between_comments_query[i,])


DT::datatable(data = data.frame(id=rownames(similarity_between_comments_query)[i], categorie_appel_clean = categorie_appel[i], comments_from_agents[i,c("Commentaires_de_lagent")], similarity=similarity_between_comments_query[i,1]), rownames = T, filter="none", selection="none", options = list(pageLength = 50, scrollX=T), caption=paste("Commentaires les plus similaires à la requête : ", query, ""))
```

